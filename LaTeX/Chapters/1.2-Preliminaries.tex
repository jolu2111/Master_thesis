\chapter{Preliminaries}
\label{ch:preliminaries}

\section{Uncertainty Quantification}

In the domain reliability analysis, the primary goal is to estimate the probability of failure, or probability of exceeding a certain threshold. Theoretically, the probability of failure is defined as the probability that the limit state function is less than zero. The limit state function is a function of the random variables and the model parameters. The random variables are the input variables to the model, and the model parameters are the parameters of the model. The limit state function is defined as $g(\mathbf{X}, \mathbf{Z})$, where $\mathbf{X}$ is the random variables and $\mathbf{Z}$ is the model parameters. The probability of failure is defined as the integral of the joint probability density function of the random variables and the model parameters over the region where the limit state function is less than zero. The probability of failure is given by the following equation: 

\begin{equation}
P_f = \int_{\Omega} f_{\mathbf{X}, \mathbf{Z}}(\mathbf{x}, \mathbf{z}) \, d\mathbf{x} \, d\mathbf{z}
\end{equation}
where $\Omega = \{ (\mathbf{x}, \mathbf{z}) \mid g(\mathbf{x}, \mathbf{z}) < 0 \}$.
% This formulation generalizes the traditional reliability analysis by incorporating both the random variables ð‘‹ (e.g., loads, material properties) and the model parameters ð‘ (e.g., geometric properties, environmental factors) into the limit state function.

This integral is often intractable and must be approximated using numerical methods. The most common method for approximating the probability of failure is the Monte Carlo simulation. The Monte Carlo simulation is a statistical method that uses random sampling to estimate the probability of failure. The Monte Carlo simulation works by generating random samples of the random variables and the model parameters, evaluating the limit state function for each sample, and counting the number of samples where the limit state function is less than zero. The probability of failure is then estimated as the ratio of the number of samples where the limit state function is less than zero to the total number of samples. The Monte Carlo simulation is a powerful and versatile method for estimating the probability of failure, but the convergence rate of MCS in number of samples is very slow. 

\begin{quotation}
    Based on the discussion above, it is safe to conclude that the primary bottleneck of all the reliability analysis techniques is the need for running simulation to evaluate the limit-state function. Often, the limit-state function are in form of complex nonlinear ordinary/partial differential equations (ODE/PDE) and solving it repeatedly can make the process computationally expensive. \citep{chakraborty2020simulationfreereliabilityanalysis}
\end{quotation}


\section{PINNs Historical Overview}
\label{sec:historical-overview}

PINNs were developed by \cite{raissi2019pinns} in 2019. The use areas are diverse, including but not limited to...
The main work of using PINNs concerns the soling of ordinary and partial differential equations (PDEs) and inverse problems governed by PDEs. The main idea is to use the PDE as a constraint in the loss function of the neural network. This is done by enforcing the PDE at a set of collocation points. The collocation points are sampled from the domain of interest. The loss function is minimized by using gradient-based optimization algorithms, as explained in \ref{ch:theory}. By relying on a PDE as a constraint, the need for labelled data is eliminated. This is a significant advantage of PINNs compared to traditional machine learning methods. 

The first work applying PINNs to reliability analysis was done by \citet{chakraborty2020simulationfreereliabilityanalysis}. 

The probabilistic model with metamodel-based importance sampling was proposed by and when....

Uncertainty quantifications in PINNs are at what stage?...
\begin{quotation}
    \textit{The current work on the application of PINN to reliability analysis is still in the preliminary stage and mainly based on the first approach, i.e., physics-informed loss function. Chakraborty (2020) firstly applied PINN to reliability analysis. In his work, the LSF is expressed in the form of linear or nonlinear PDEs, and PINN is used to solve PDEs and obtain the failure probability. Based on the work in Chakraborty (2020), Zhang and Shafieezadeh (2022) introduced an active learning approach with the dual objective of training PINN for solving PDEs and characterizing the limit state. Zhou et al. (2023) applied PINN to the dynamic reliability analysis of multi-state systems. All these works transformed the reliability analysis problems into the solution of PDEs. However, the PDEs are not always available for reliability problems especially for static reliability, and PINN proposed in Raissi et al. (2019) is not available for such reliability problems that cannot be described by PDEs. For such problems, a novel NN approach combined with physical information needs to be constructed for reliability analysis.} 
    \cite{bai2023reliability}
\end{quotation}

The fact that PINNs are not always available for reliability problems that cannot be described by PDEs is a significant limitation. This is a motivation for the current work. For static reliability problems, the system behaviour is typically modeled using limit state functions like g(X), where X is the random variables. This is an algebraic equation, with no derivatives. 



B-PINNs and E-PINNs and P-PINNs all exist..

The combination of PINNs and metamodel-based importance sampling is novel and has not been explored before. 

