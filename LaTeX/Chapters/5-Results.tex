\chapter{Results}
\label{ch:results}

I have tried to have time linspaced and shufled, and also heavytailed both ways, but nothing is better then just having it ranomdly distributed. 

\section{Time Normalisation Challenges}
\label{sec:time_norm_challenges}

\subsection{Pitfalls of Time Normalisation in Physics--Informed Neural Networks}
\label{sec:time_norm_pinn}

%---------------------------------------------------------------------------
\paragraph{Motivation.}
Normalising the temporal domain to $\tau\in[0,1]$ is attractive because it
reduces the dynamic range of the inputs fed into the network and allows the
same collocation grid to be reused for different physical problems.  
In a PINN, however, temporal rescaling \emph{alters the size of every time
derivative} that appears in the loss function.  When this interacts with
feature normalisation of other parameters (\textit{e.g.,} $m,\mu,v_0$) and with
saturating activations such as $\tanh$, the result can be
severe degradation of training for large values of the normalised
parameters.  This subsection formalises the effect and documents the empirical
behaviour we observed.

%---------------------------------------------------------------------------
\subsubsection{Mathematical analysis}

Let $t\_{\text{phys}}\in[0,T]$ denote physical time and define the normalised
variable
\begin{equation}
 \tau \;=\;\frac{t\_{\text{phys}}}{T}\;\in[0,1],\qquad T>0.
 \label{eq:tau_def}
\end{equation}
Assume the PINN learns a single scalar state $y(\tau)$, where the physical
(state) is related by
$y(\tau)=y\_{\text{phys}}\!\bigl(t\_{\text{phys}}=T\tau\bigr)$.
Applying the chain rule yields
\begin{align}
 \frac{\partial y}{\partial\tau} &= T\,\frac{\partial
 y\_{\text{phys}}}{\partial t\_{\text{phys}}}, &
 \frac{\partial^2 y}{\partial\tau^{2}} &= T^{2}\,
 \frac{\partial^{2}y\_{\text{phys}}}{\partial t\_{\text{phys}}^{2}}.
 \label{eq:chain_rule}
\end{align}

\paragraph{Boundary–condition scaling.}
For a damped oscillator we enforce
\begin{equation}
    y\_{\text{phys}}(0)=y_0, 
    \qquad 
    \dot y\_{\text{phys}}(0)=v_0.
\end{equation}
Expressed in $\tau$,
\begin{equation}
 y(0)=y_0,
 \qquad
 \frac{\partial y}{\partial \tau}(0)=T\,v_0.
 \label{eq:bc_tau}
\end{equation}
Hence \textbf{the derivative target is multiplied by $T$}.  For
$T=5\,\text{s}$ and $v_0=5.5\,\text{m\,s}^{-1}$ the network must
produce a slope of $27.5$ at~$\tau=0$.

\paragraph{Effect on the network.}
Consider the first hidden layer
\begin{equation}
 z^{(1)} \;=\; W^{(1)}x + b^{(1)}, 
 \qquad 
 x=[\tau,m_z,\mu_z,k_z,y_{0,z},v_{0,z}]^\top,
 \quad 
 a^{(1)}=\tanh\!\bigl(z^{(1)}\bigr).
\end{equation}
All \emph{z–scored} inputs $m_z,\mu_z,v_{0,z}$ are
distributed over $\approx[-5,5]$ in the training data.
With weights of order $\mathcal O(1)$ the pre--activations quickly reach
$|z^{(1)}|>3$, pushing $\tanh$ into saturation where
$\tanh'(z^{(1)})\approx0$ and gradients vanish.
The large derivative requirement
\eqref{eq:bc_tau} forces even larger weights in deeper layers, reinforcing
the saturation.

%---------------------------------------------------------------------------
\subsubsection{Empirical demonstration}

\begin{figure}[ht]
  \centering
  %\includegraphics[width=0.8\linewidth]{\detokenize{fig/pinn_loss_vs_epoch.pdf}}
  \caption{%
    \textbf{Training loss for three configurations.}
    (i) Unnormalised time and parameters (blue);
    (ii) normalised time, but only $v_0$ z–scored (orange); 
    (iii) normalised time \emph{and} three parameters
    $m,\mu,v_0$ z–scored (green).
    The third setting stalls after $\approx10^3$ epochs.
    \emph{[Insert figure from experiment]}}
  \label{fig:loss_norm_effect}
\end{figure}

Figure~\ref{fig:loss_norm_effect} shows the training loss for three
set--ups.\footnote{%
Exact hyper–parameters are given in Table~\ref{tab:pinn_hparams}.}
Configuration~(iii) collapses once the joint sample
$m_z\!=\!\mu_z\!=\!v_{0,z}\!=\!5$ is encountered; pre--activation statistics
for the first layer become
$\min z=-8.7$, $\max z=7.1$,
confirming saturation.

%---------------------------------------------------------------------------
\subsubsection{Mitigation strategies}

\begin{enumerate}[label=(\roman*)]
  \item \textbf{Clip feature range:} restrict each z--score to
        $\lvert x_z\rvert\le 3$ so that $|z^{(1)}|\lesssim3$.
  \item \textbf{Learnable input scaling:} prepend a $6\times6$
        linear layer (no bias) that adapts per--feature gains during
        training.
  \item \textbf{Switch activation:} replace $\tanh$ with
        $\mathrm{SiLU}$ or $\mathrm{ReLU}$ to avoid saturation.
  \item \textbf{Stay in physical time:} forego normalising $t$
        entirely; then the derivative target remains $\dot y(0)=v_0$
        and weight magnitudes stay moderate.
\end{enumerate}

Table~\ref{tab:mitigation_results} summarises test--error statistics after
applying each remedy.

%---------------------------------------------------------------------------
\subsubsection{Summary}

Normalising the temporal axis in a PINN scales every time derivative by $T$.
If several additional inputs are z–scored into a wide range (\textit{e.g.,} $\pm5\sigma$)
and activations with bounded output ($\mathrm{tanh}$) are used, the \emph{combined} effect
is to drive early--layer pre--activations far outside the linear region,
causing vanishing gradients and failure to fit boundary conditions for large
parameter values.  Mitigations include clipping the input range, letting the
network learn its own per--feature scale, or replacing $\tanh$ by a non--saturating activation function.
non--saturating activation.

%---------------------------------------------------------------------------
% Example placeholders for tables you will fill in later
\begin{table}[ht]
  \centering
  \caption{Hyper--parameters used in the experiments.}
  \label{tab:pinn_hparams}
  \begin{tabular}{lcc}
    \toprule
    Parameter & Value & Notes\\
    \midrule
    Hidden layers & 3 & 20 neurons each\\
    Activation & \texttt{tanh} & unless stated otherwise\\
    Optimiser & Adam & $\eta=10^{-3}$\\
    Training samples & 500 & per epoch\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
  \centering
  \caption{Relative $\mathcal L_2$ error on an extreme test case
    ($m_z=\mu_z=v_{0,z}=5$).}
  \label{tab:mitigation_results}
  \begin{tabular}{lcc}
    \toprule
    Configuration & Test error & Converged?\\
    \midrule
    Baseline (norm.\ time, 3 z--scored features) & $3.1\mathrm{e}{-1}$ & no\\
    Clipped inputs ($\pm3\sigma$) & \emph{[fill]} & yes\\
    SiLU activation & \emph{[fill]} & yes\\
    Learnable scaler + tanh & \emph{[fill]} & yes\\
    Physical time (no $t$ norm.) & \emph{[fill]} & yes\\
    \bottomrule
  \end{tabular}
\end{table}
