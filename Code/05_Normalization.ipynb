{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbimporter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbimporter\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nbimporter'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "# Define the PINN model\n",
    "class PINN_fixed(nn.Module):\n",
    "    def __init__(self, hidden_size=20, hidden_layers=3, dropout_rate=0.05):\n",
    "        super(PINN_fixed, self).__init__()\n",
    "        input_dim = 6\n",
    "        layers = [nn.Linear(input_dim, hidden_size), nn.Tanh()]\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, t, m, mu, k, y0, v0):\n",
    "        N = t.shape[0]\n",
    "        m_  = m.expand(N, -1)\n",
    "        mu_ = mu.expand(N, -1)\n",
    "        k_  = k.expand(N, -1)\n",
    "        y0_ = y0.expand(N, -1)\n",
    "        v0_ = v0.expand(N, -1)\n",
    "        x = torch.cat([t, m_, mu_, k_, y0_, v0_], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Compute PDE loss\n",
    "def pde_loss(model, t, m, mu, k, y0_val, v0_val):\n",
    "    y = model(t, m, mu, k, y0_val, v0_val)\n",
    "    y_t = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    y_tt = torch.autograd.grad(y_t, t, grad_outputs=torch.ones_like(y_t), create_graph=True)[0]\n",
    "    residual = m * y_tt + mu * y_t + k * y\n",
    "    return torch.mean(residual**2)\n",
    "\n",
    "# Compute boundary loss\n",
    "def boundary_loss(model, t0, m, mu, k, y0, v0):\n",
    "    y_pred = model(t0, m, mu, k, y0, v0)\n",
    "    y_t = torch.autograd.grad(y_pred, t0, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    return torch.mean((y_pred - y0)**2 + (y_t - v0)**2)\n",
    "\n",
    "def plot_loss(epoch, losses_dict):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    for loss_name, loss_values in losses_dict.items():\n",
    "        plt.plot(epoch, loss_values, label=loss_name)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def z_score_normalize(x):\n",
    "    z= x\n",
    "\n",
    "# Trainer class to manage training process\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, epochs=4001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.losses = {\"Residual Loss\": [], \"Boundary Loss\": []}\n",
    "        self.lambda_bc = 10.0\n",
    "    \n",
    "    def train(self, t_coll, m_val, mu_val, k_val, y0_val, v0_val, t0):\n",
    "        best_loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_pde = pde_loss(self.model, t_coll, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "            loss_bc = boundary_loss(self.model, t0, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "            loss = loss_pde + self.lambda_bc * loss_bc\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losses[\"Residual Loss\"].append(loss_pde.item())\n",
    "            self.losses[\"Boundary Loss\"].append(loss_bc.item())\n",
    "            current_loss = loss.item()\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, PDE loss: {loss_pde.item()}, BC loss: {loss_bc.item()}\")\n",
    "                plot_loss(range(epoch + 1), self.losses)\n",
    "        print(f\"Phase 1 complete. Best loss so far: {best_loss}\")\n",
    "\n",
    "            # Phase 2: Continue training until an epoch is reached with loss below the best_loss from Phase 1.\n",
    "        extra_epochs = 0\n",
    "        while True:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_pde = pde_loss(self.model, t_coll, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "            loss_bc = boundary_loss(self.model, t0, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "            loss = loss_pde + self.lambda_bc * loss_bc\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            extra_epochs += 1\n",
    "            epoch += 1\n",
    "            current_loss = loss.item()\n",
    "            self.losses[\"Residual Loss\"].append(loss_pde.item())\n",
    "            self.losses[\"Boundary Loss\"].append(loss_bc.item())\n",
    "\n",
    "            if extra_epochs % 1000 == 0:\n",
    "                print(f\"Extra Epoch {extra_epochs}, PDE loss: {loss_pde.item()}, BC loss: {loss_bc.item()}\")\n",
    "                plot_loss(range(epoch + 1), self.losses)\n",
    "            if current_loss < best_loss:\n",
    "                print(f\"Improved loss found: {current_loss} (after {extra_epochs} extra epochs)\")\n",
    "                plot_loss(range(epoch + 1), self.losses)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def z_score_normalize(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "def initialize_uniform_mass(N):\n",
    "    # Generate random time collocation points scaled to [0, 5]\n",
    "    t_coll = torch.rand(N, 1) * 5\n",
    "    t_coll.requires_grad_(True)\n",
    "    \n",
    "    # Define mass parameters and sample mass from a uniform distribution\n",
    "    m_mean, m_std = 1.0, 0.1\n",
    "    m_coll = torch.FloatTensor(N, 1).uniform_(m_mean - 2 * m_std, m_mean + 8 * m_std)\n",
    "    m_coll.requires_grad_(True)\n",
    "    \n",
    "    # Apply Z-score normalization to the mass collocation points\n",
    "    m_coll = z_score_normalize(m_coll, m_mean, m_std)\n",
    "    \n",
    "    # Define the remaining parameters as scalars (with gradients enabled)\n",
    "    mu, k, y0, v0 = 0.6, 5.0, -0.4, 3.0\n",
    "    mu_val = torch.tensor([[mu]], requires_grad=True)\n",
    "    k_val = torch.tensor([[k]], requires_grad=True)\n",
    "    y0_val = torch.tensor([[y0]], requires_grad=True)\n",
    "    v0_val = torch.tensor([[v0]], requires_grad=True)\n",
    "    \n",
    "    # Define a single time for the boundary condition\n",
    "    t0 = torch.zeros_like(t_coll).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return t_coll, m_coll, mu_val, k_val, y0_val, v0_val, t0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
