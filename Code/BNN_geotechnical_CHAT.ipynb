{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyro-ppl --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Enable deterministic behavior for reproducibility (optional)\n",
    "pyro.set_rng_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True function: h(x) = 1 + 3x\n",
    "def true_head(x):\n",
    "    return 1.0 + 3.0 * x\n",
    "\n",
    "# Generate data\n",
    "num_points = 20\n",
    "X = torch.linspace(0, 1, num_points)\n",
    "h_true = true_head(X)\n",
    "\n",
    "# Add synthetic measurement noise\n",
    "noise_std = 0.1\n",
    "h_obs = h_true + noise_std * torch.randn(num_points)\n",
    "\n",
    "# Reshape for neural network usage\n",
    "X = X.unsqueeze(-1)       # shape [20, 1]\n",
    "h_obs = h_obs.unsqueeze(-1)  # shape [20, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_forward(x, weights, biases):\n",
    "    \"\"\"\n",
    "    Forward pass of a simple 2-layer network:\n",
    "      hidden layer => ReLU => output layer\n",
    "    weights, biases are dictionaries containing:\n",
    "      - w1, b1 for layer1\n",
    "      - w2, b2 for layer2\n",
    "    x is shape [batch_size, input_dim]\n",
    "    \"\"\"\n",
    "    # Hidden layer\n",
    "    h = torch.relu(torch.mm(x, weights['w1']) + biases['b1'])\n",
    "    # Output layer\n",
    "    out = torch.mm(h, weights['w2']) + biases['b2']\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10  # number of neurons in hidden layer\n",
    "\n",
    "def model(x, y_obs=None):\n",
    "    # PRIOR: Define Normal(0,1) over weights & biases (mean=0, std=1)\n",
    "    # Layer 1\n",
    "    w1 = pyro.sample(\"w1\", dist.Normal(torch.zeros(1, hidden_dim), torch.ones(1, hidden_dim)))\n",
    "    b1 = pyro.sample(\"b1\", dist.Normal(torch.zeros(hidden_dim), torch.ones(hidden_dim)))\n",
    "    \n",
    "    # Layer 2\n",
    "    w2 = pyro.sample(\"w2\", dist.Normal(torch.zeros(hidden_dim, 1), torch.ones(hidden_dim, 1)))\n",
    "    b2 = pyro.sample(\"b2\", dist.Normal(torch.zeros(1), torch.ones(1)))\n",
    "    \n",
    "    # Predictions using the forward pass\n",
    "    weights = {'w1': w1, 'w2': w2}\n",
    "    biases  = {'b1': b1, 'b2': b2}\n",
    "    y_pred = bnn_forward(x, weights, biases)\n",
    "    \n",
    "    # LIKELIHOOD: We assume Gaussian noise around predicted y\n",
    "    sigma = pyro.sample(\"sigma\", dist.Exponential(torch.tensor(1.0)))  # noise scale\n",
    "\n",
    "    # Condition on observed data if provided\n",
    "    with pyro.plate(\"data_plate\", x.shape[0]):\n",
    "        pyro.sample(\"obs\", dist.Normal(y_pred, sigma), obs=y_obs)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x, y_obs=None):\n",
    "    # For each prior, define variational distribution parameters\n",
    "    w1_loc = pyro.param(\"w1_loc\", torch.randn(1, hidden_dim))\n",
    "    w1_scale = pyro.param(\"w1_scale\", torch.ones(1, hidden_dim), constraint=torch.distributions.constraints.positive)\n",
    "    \n",
    "    b1_loc = pyro.param(\"b1_loc\", torch.randn(hidden_dim))\n",
    "    b1_scale = pyro.param(\"b1_scale\", torch.ones(hidden_dim), constraint=torch.distributions.constraints.positive)\n",
    "    \n",
    "    w2_loc = pyro.param(\"w2_loc\", torch.randn(hidden_dim, 1))\n",
    "    w2_scale = pyro.param(\"w2_scale\", torch.ones(hidden_dim, 1), constraint=torch.distributions.constraints.positive)\n",
    "    \n",
    "    b2_loc = pyro.param(\"b2_loc\", torch.randn(1))\n",
    "    b2_scale = pyro.param(\"b2_scale\", torch.ones(1), constraint=torch.distributions.constraints.positive)\n",
    "\n",
    "    sigma_loc = pyro.param(\"sigma_loc\", torch.tensor(1.0), constraint=torch.distributions.constraints.positive)\n",
    "\n",
    "    # SAMPLE from these distributions\n",
    "    pyro.sample(\"w1\", dist.Normal(w1_loc, w1_scale))\n",
    "    pyro.sample(\"b1\", dist.Normal(b1_loc, b1_scale))\n",
    "    pyro.sample(\"w2\", dist.Normal(w2_loc, w2_scale))\n",
    "    pyro.sample(\"b2\", dist.Normal(b2_loc, b2_scale))\n",
    "    pyro.sample(\"sigma\", dist.Exponential(sigma_loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Pyro parameters\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam({\"lr\": 0.01})\n",
    "\n",
    "# Define our SVI object\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "num_iterations = 3000\n",
    "losses = []\n",
    "\n",
    "for step in range(num_iterations):\n",
    "    loss = svi.step(X, h_obs)\n",
    "    losses.append(loss)\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}, Loss = {loss:.2f}\")\n",
    "\n",
    "# Plot the convergence of ELBO loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO Loss during training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of posterior samples\n",
    "num_samples = 200\n",
    "\n",
    "x_plot = torch.linspace(0, 1, 100).unsqueeze(-1)\n",
    "predictions = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    # sample from the guide\n",
    "    w1_sample = dist.Normal(pyro.param(\"w1_loc\"), pyro.param(\"w1_scale\")).sample()\n",
    "    b1_sample = dist.Normal(pyro.param(\"b1_loc\"), pyro.param(\"b1_scale\")).sample()\n",
    "    w2_sample = dist.Normal(pyro.param(\"w2_loc\"), pyro.param(\"w2_scale\")).sample()\n",
    "    b2_sample = dist.Normal(pyro.param(\"b2_loc\"), pyro.param(\"b2_scale\")).sample()\n",
    "    sigma_sample = dist.Exponential(pyro.param(\"sigma_loc\")).sample()\n",
    "\n",
    "    weights_samp = {'w1': w1_sample, 'w2': w2_sample}\n",
    "    biases_samp  = {'b1': b1_sample, 'b2': b2_sample}\n",
    "    \n",
    "    y_pred_samp = bnn_forward(x_plot, weights_samp, biases_samp).detach().squeeze()\n",
    "    predictions.append(y_pred_samp.numpy())\n",
    "\n",
    "predictions = np.array(predictions)  # shape [num_samples, 100]\n",
    "mean_pred = np.mean(predictions, axis=0)\n",
    "std_pred = np.std(predictions, axis=0)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X.squeeze(), h_obs.squeeze(), 'o', label=\"Observations\")\n",
    "plt.plot(x_plot.squeeze(), true_head(x_plot).squeeze(), 'k--', label=\"True function\")\n",
    "\n",
    "plt.plot(x_plot.squeeze(), mean_pred, 'r', label=\"BNN Mean Prediction\")\n",
    "plt.fill_between(x_plot.squeeze(),\n",
    "                 mean_pred - 2*std_pred,\n",
    "                 mean_pred + 2*std_pred,\n",
    "                 alpha=0.2,\n",
    "                 color=\"red\",\n",
    "                 label=\"Â±2 SD\")\n",
    "\n",
    "plt.title(\"Bayesian Neural Network on 1D Flow Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Head h(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
