{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "EPS = 1e-5  # define a small constant for numerical stability control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_func(x):\n",
    "    return np.sin(x * math.pi / 2 + 0.8) * np.exp(-0.1 * np.abs(x)) + 0.1 * x\n",
    "\n",
    "def gen_data(N_data, ground_truth_func, noise_std=None): \n",
    "    # generate the training dataset, note here we will make data into 2 clusters\n",
    "    x1 = np.random.randn(int(N_data/2), 1) * 0.5 + 2.0\n",
    "    x2 = np.random.randn(int(N_data/2), 1) * 0.5 - 2.0\n",
    "    x = np.concatenate([x1, x2], axis=0)\n",
    "    y = ground_truth_func(x)\n",
    "    if noise_std is not None and noise_std > EPS:\n",
    "        # assume homogeneous noise setting, i.e., \"homoscedasticity\"\n",
    "        y += np.random.randn(y.shape[0], y.shape[1]) * noise_std\n",
    "    return x, y\n",
    "\n",
    "def normalise_data(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "def unnormalise_data(x, mean, std):\n",
    "    return x * std + mean\n",
    "\n",
    "class regression_data(Dataset):\n",
    "     def __init__(self, x, y, normalise=True):\n",
    "         super(regression_data, self).__init__()\n",
    "         self.update_data(x, y, normalise)\n",
    "\n",
    "     def __len__(self):\n",
    "         return self.x.shape[0]\n",
    "\n",
    "     def __getitem__(self, index):\n",
    "         x = torch.tensor(self.x[index]).float()\n",
    "         y = torch.tensor(self.y[index]).float()\n",
    "         return x, y\n",
    "\n",
    "     def update_data(self, x, y, normalise=True, update_stats=True):\n",
    "         assert x.shape[0] == y.shape[0]\n",
    "         self.x = x\n",
    "         self.y = y\n",
    "         # normalise data\n",
    "         self.normalise = normalise\n",
    "         if update_stats:\n",
    "             self.x_mean = self.x.mean(0) if normalise else 0.0\n",
    "             self.x_std = self.x.std(0) if normalise else 1.0\n",
    "             self.y_mean = self.y.mean(0) if normalise else 0.0\n",
    "             self.y_std = self.y.std(0) if normalise else 1.0\n",
    "         if self.normalise:\n",
    "             self.x = normalise_data(self.x, self.x_mean, self.x_std)\n",
    "             self.y = normalise_data(self.y, self.y_mean, self.y_std)\n",
    "\n",
    "N_data = 100\n",
    "noise_std = 0.1\n",
    "x_train, y_train = gen_data(N_data, ground_truth_func, noise_std)\n",
    "dataset = regression_data(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# plot the training data and ground truth\n",
    "x_test = np.arange(np.min(x_train) - 1.0, np.max(x_train)+1.0, 0.01)[:, np.newaxis]\n",
    "y_test = ground_truth_func(x_test)\n",
    "plt.plot(x_train, y_train, 'ro', label='data')\n",
    "plt.plot(x_test, y_test, 'k-', label='ground-truth')\n",
    "plt.legend()\n",
    "plt.title('ground-truth function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    # y should have shape as (batch_size, y_dim)\n",
    "    # y_pred should have shape as (batch_size, y_dim) or (K, batch_size, y_dim)\n",
    "    # where K is the number of MC samples\n",
    "    # this function should return per-data loss of shape (batch_size,) or (K, batch_size)\n",
    "    ### begin of your code ###\n",
    "    # hint: consult with your textbook or wikipedia for the Gaussian distribution form\n",
    "    l2_dist=(y-y_pred).pow(2).sum(-1)\n",
    "    ll = -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "    ### end of your code ###\n",
    "    return ll\n",
    "\n",
    "# we assume a Gaussian likelihood with homogeneuous noise\n",
    "log_noise_var = nn.Parameter(torch.ones(size=(), device=device)*-3.0)\n",
    "\n",
    "data_loss_func = lambda y, y_pred: -gauss_loglik(y, y_pred, log_noise_var)\n",
    "nll = data_loss_func(y, y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib.ticker import LogLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # Numpy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # All GPUs (if multiple)\n",
    "\n",
    "# Set the seed\n",
    "set_seed(123)\n",
    "\n",
    "# Define the neural network for the Physics-Informed Neural Network (PINN)\n",
    "class pinn(nn.Module):\n",
    "    def __init__(self, hidden_size=20, hidden_layers=3):\n",
    "        super(pinn, self).__init__()\n",
    "        layers = [nn.Linear(2, hidden_size), nn.Tanh()]  # Change Tanh to GeLU\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())  # Change Tanh to GeLU\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def laplace_residual(model, coords):\n",
    "    u = model(coords)\n",
    "    grads = torch.autograd.grad(u, coords, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(grads[:, 0], coords, grad_outputs=torch.ones_like(grads[:, 0]), create_graph=True)[0][:, 0]\n",
    "    u_yy = torch.autograd.grad(grads[:, 1], coords, grad_outputs=torch.ones_like(grads[:, 1]), create_graph=True)[0][:, 1]\n",
    "    return torch.mean((u_xx + u_yy) ** 2)\n",
    "\n",
    "#Boundary conditions:\n",
    "def loss_dirichlet_boundary(model,boundary_points,boundary_target):\n",
    "    u_pred = model(boundary_points)\n",
    "    boundary_residual= u_pred-boundary_target\n",
    "    return torch.mean((boundary_residual)**2)\n",
    "\n",
    "def loss_neumann_boundary(model,boundary_points,axis='horizontal'):\n",
    "    u_pred = model(boundary_points)\n",
    "    u_grad= torch.autograd.grad(u_pred,boundary_points,grad_outputs=torch.ones_like(u_pred),create_graph=True)[0] #This is the gradient of u \n",
    "    if axis=='vertical':\n",
    "        u_x = u_grad[:,0] # This is the x-component of the gradient of u\n",
    "        return torch.mean((u_x)**2)\n",
    "    elif axis=='horizontal':\n",
    "        u_y = u_grad[:,1] # This is the y-component of the gradient of u\n",
    "        return torch.mean((u_y)**2)\n",
    "\n",
    "# Function to create boundary points\n",
    "def create_boundary_points(axis, place, start, end, num_points, head=None):\n",
    "    line_points = torch.full((num_points, 1), place)\n",
    "    place_points = start + (end - start) * torch.rand(num_points, 1)\n",
    "    \n",
    "    if axis == 'vertical':\n",
    "        boundary_points = torch.cat([line_points, place_points], dim=1)\n",
    "    elif axis == 'horizontal':\n",
    "        boundary_points = torch.cat([place_points, line_points], dim=1)\n",
    "\n",
    "    # Only return targets if head value is provided (for Dirichlet conditions)\n",
    "    if head is not None:\n",
    "        boundary_targets = torch.full((num_points, 1), head, dtype=torch.float)\n",
    "        return boundary_points, boundary_targets\n",
    "    else:\n",
    "        return boundary_points\n",
    "\n",
    "def plot_losses(epochs, losses_dict):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for loss_name, loss_values in losses_dict.items():\n",
    "        plt.plot(epochs, loss_values, label=loss_name)\n",
    "    plt.gca().yaxis.set_major_locator(LogLocator(base=10.0))\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    # Compute individual loss terms\n",
    "    loss_laplace = laplace_residual(model, train_points)\n",
    "    loss_d_l = loss_dirichlet_boundary(model, boundary_points_dirichlet_left, target_points_dirichlet_left)\n",
    "    loss_d_r = loss_dirichlet_boundary(model, boundary_points_dirichlet_right, target_points_dirichlet_right)\n",
    "    loss_d_tl = loss_dirichlet_boundary(model, boundary_points_dirichlet_topleft, target_points_dirichlet_topleft)\n",
    "    loss_d_tr = loss_dirichlet_boundary(model, boundary_points_dirichlet_topright, target_points_dirichlet_topright)\n",
    "    loss_n_t = loss_neumann_boundary(model, boundary_points_neumann_top, axis='horizontal')\n",
    "    loss_n_b = loss_neumann_boundary(model, boundary_points_neumann_bottom, axis='horizontal')\n",
    "    \n",
    "    # Combine losses using the updated weights\n",
    "    loss = (\n",
    "        loss_weights[0] * loss_laplace +\n",
    "        loss_weights[1] * loss_d_l +\n",
    "        loss_weights[2] * loss_d_r +\n",
    "        loss_weights[3] * loss_d_tl +\n",
    "        loss_weights[4] * loss_d_tr +\n",
    "        loss_weights[5] * loss_n_t +\n",
    "        loss_weights[6] * loss_n_b\n",
    "    )\n",
    "    \n",
    "    # Record losses and weights\n",
    "    laplace_losses.append(loss_laplace.item())\n",
    "    dirichlet_losses_left.append(loss_d_l.item())\n",
    "    dirichlet_losses_right.append(loss_d_r.item())\n",
    "    dirichlet_losses_topleft.append(loss_d_tl.item())\n",
    "    dirichlet_losses_topright.append(loss_d_tr.item())\n",
    "    neumann_losses_top.append(loss_n_t.item())\n",
    "    neumann_losses_bottom.append(loss_n_b.item())\n",
    "    epochs_recorded.append(epoch)\n",
    "\n",
    "    return loss\n",
    "\n",
    "#_____________________________________________________________________________________________________________\n",
    "\n",
    "#Domain size:\n",
    "left, right, bottom, top = 0, 1, 0, 0.2\n",
    "\n",
    "left_dirichlet,target_left_dirichlet = create_boundary_points('vertical', left, 0, top, 1000, 1)  # Left boundary\n",
    "boundary_points_dirichlet_left=torch.cat([left_dirichlet],dim=0).requires_grad_(True)  # Concatenate Dirichlet boundary points\n",
    "target_points_dirichlet_left=torch.cat([target_left_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary targets\n",
    "\n",
    "right_dirichlet, target_right_dirichlet = create_boundary_points('vertical', right, 0, top, 1000, 0) # Right boundary\n",
    "boundary_points_dirichlet_right=torch.cat([right_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary points\n",
    "target_points_dirichlet_right=torch.cat([target_right_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary targets\n",
    "\n",
    "topleft_dirichlet, target_topleft_dirichlet= create_boundary_points('horizontal', top, 0, 0.3*right, 3000, 1)\n",
    "boundary_points_dirichlet_topleft=torch.cat([topleft_dirichlet],dim=0).requires_grad_(True)\n",
    "target_points_dirichlet_topleft=torch.cat([target_topleft_dirichlet],dim=0).requires_grad_(True)\n",
    "\n",
    "topright_dirichlet,target_topright_dirichlet = create_boundary_points('horizontal', top, 0.7*right, 1*right, 3000, 0)\n",
    "boundary_points_dirichlet_topright=torch.cat([topright_dirichlet],dim=0).requires_grad_(True)\n",
    "target_points_dirichlet_topright=torch.cat([target_topright_dirichlet],dim=0).requires_grad_(True)\n",
    "\n",
    "top_neumann=create_boundary_points('horizontal', top, 0.3*right, 0.7*right, 3000)\n",
    "boundary_points_neumann_top=torch.cat([top_neumann],dim=0).requires_grad_(True)\n",
    "\n",
    "bottom_neumann=create_boundary_points('horizontal', bottom, 0, right, 1000)\n",
    "boundary_points_neumann_bottom=torch.cat([bottom_neumann],dim=0).requires_grad_(True)\n",
    "\n",
    "\n",
    "# Creating domain points\n",
    "num_domain_points=6500\n",
    "x_points = torch.rand(num_domain_points,1) \n",
    "x = (x_points*right).requires_grad_(True)\n",
    "y_points = torch.rand(num_domain_points,1)\n",
    "y=(y_points*top).requires_grad_(True)\n",
    "train_points=torch.cat([x,y],dim=1) \n",
    "\n",
    "# Initialize lists to store losses and plot\n",
    "laplace_losses = []\n",
    "dirichlet_losses_left = []\n",
    "dirichlet_losses_right = []\n",
    "dirichlet_losses_topleft = []\n",
    "dirichlet_losses_topright = []\n",
    "neumann_losses_top = []\n",
    "neumann_losses_bottom = []\n",
    "epochs_recorded = []\n",
    "\n",
    "\n",
    "#Defining hyperparameters:\n",
    "model=pinn(hidden_size=20, hidden_layers=8) \n",
    "\n",
    "epochs=2001\n",
    "\n",
    "# Initialize the loss weights\n",
    "lambda_laplace, y_d_l, y_d_r, y_d_tl, y_d_tr, y_n_t, y_n_b = 2,100,100,1000,1000,100,100\n",
    "loss_weights = (torch.tensor([lambda_laplace, y_d_l, y_d_r, y_d_tl, y_d_tr, y_n_t, y_n_b], \n",
    "                            dtype=torch.float, requires_grad=False))\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the weights and biases of the model\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)  # Use Xavier initialization\n",
    "        nn.init.zeros_(m.bias)  # Initialize biases to zero\n",
    "# Apply weight initialization\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "\n",
    "#Training loop:\n",
    "for epoch in range(epochs):\n",
    "    adam_optimizer.zero_grad()\n",
    "    loss=closure()\n",
    "    loss.backward()\n",
    "    adam_optimizer.step() # Update the weights and biases of the model\n",
    "\n",
    "    # Logging\n",
    "    if epoch % 6000 == 0:\n",
    "        losses_dict = {\n",
    "            \"Laplace Loss\": laplace_losses,\n",
    "            \"Dirichlet Loss Left\": dirichlet_losses_left,\n",
    "            \"Dirichlet Loss Right\": dirichlet_losses_right,\n",
    "            \"Dirichlet Loss Topleft\": dirichlet_losses_topleft,\n",
    "            \"Dirichlet Loss Topright\": dirichlet_losses_topright,\n",
    "            \"Neumann Loss Top\": neumann_losses_top,\n",
    "            \"Neumann Loss Bottom\": neumann_losses_bottom\n",
    "        }\n",
    "\n",
    "        plot_losses(epochs_recorded, losses_dict)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dict = {\n",
    "    \"Laplace Loss\": laplace_losses,\n",
    "    \"Dirichlet Loss Left\": dirichlet_losses_left,\n",
    "    \"Dirichlet Loss Right\": dirichlet_losses_right,\n",
    "    \"Dirichlet Loss Topleft\": dirichlet_losses_topleft,\n",
    "    \"Dirichlet Loss Topright\": dirichlet_losses_topright,\n",
    "    \"Neumann Loss Top\": neumann_losses_top,\n",
    "    \"Neumann Loss Bottom\": neumann_losses_bottom\n",
    "}\n",
    "\n",
    "plot_losses(epochs_recorded, losses_dict)\n",
    "print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damped_harmonic_oscillator(m, mu, k, y0, v0, t):\n",
    "    # Check underdamped condition\n",
    "    if mu**2 >= 4 * m * k:\n",
    "        raise ValueError(\"The system is not underdamped. Ensure that μ^2 < 4 * m * k.\")\n",
    "    \n",
    "    # Angular frequency\n",
    "    omega = np.sqrt(k/m - (mu/(2*m)) ** 2)\n",
    "    \n",
    "    # Exact solution\n",
    "    A = y0\n",
    "    B = (v0 + (mu/(2*m)) * y0) / omega\n",
    "    y_exact = np.exp(-mu * t / (2 * m)) * (A * np.cos(omega * t) + B * np.sin(omega * t))\n",
    "    \n",
    "    return y_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # Numpy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # All GPUs (if multiple)\n",
    "\n",
    "# Set the seed\n",
    "set_seed(123)\n",
    "\n",
    "# Define the neural network for the Physics-Informed Neural Network (PINN)\n",
    "class pinn(nn.Module):\n",
    "    def __init__(self, hidden_size=20, hidden_layers=3):\n",
    "        super(pinn, self).__init__()\n",
    "        layers = [nn.Linear(2, hidden_size), nn.Tanh()]  # Change Tanh to GeLU\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())  # Change Tanh to GeLU\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def laplace_residual(model, coords):\n",
    "    u = model(coords)\n",
    "    grads = torch.autograd.grad(u, coords, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(grads[:, 0], coords, grad_outputs=torch.ones_like(grads[:, 0]), create_graph=True)[0][:, 0]\n",
    "    u_yy = torch.autograd.grad(grads[:, 1], coords, grad_outputs=torch.ones_like(grads[:, 1]), create_graph=True)[0][:, 1]\n",
    "    return torch.mean((u_xx + u_yy) ** 2)\n",
    "\n",
    "#Boundary conditions:\n",
    "def loss_dirichlet_boundary(model,boundary_points,boundary_target):\n",
    "    u_pred = model(boundary_points)\n",
    "    boundary_residual= u_pred-boundary_target\n",
    "    return torch.mean((boundary_residual)**2)\n",
    "\n",
    "def loss_neumann_boundary(model,boundary_points,axis='horizontal'):\n",
    "    u_pred = model(boundary_points)\n",
    "    u_grad= torch.autograd.grad(u_pred,boundary_points,grad_outputs=torch.ones_like(u_pred),create_graph=True)[0] #This is the gradient of u \n",
    "    if axis=='vertical':\n",
    "        u_x = u_grad[:,0] # This is the x-component of the gradient of u\n",
    "        return torch.mean((u_x)**2)\n",
    "    elif axis=='horizontal':\n",
    "        u_y = u_grad[:,1] # This is the y-component of the gradient of u\n",
    "        return torch.mean((u_y)**2)\n",
    "\n",
    "# Function to create boundary points\n",
    "def create_boundary_points(axis, place, start, end, num_points, head=None):\n",
    "    line_points = torch.full((num_points, 1), place)\n",
    "    place_points = start + (end - start) * torch.rand(num_points, 1)\n",
    "    \n",
    "    if axis == 'vertical':\n",
    "        boundary_points = torch.cat([line_points, place_points], dim=1)\n",
    "    elif axis == 'horizontal':\n",
    "        boundary_points = torch.cat([place_points, line_points], dim=1)\n",
    "\n",
    "    # Only return targets if head value is provided (for Dirichlet conditions)\n",
    "    if head is not None:\n",
    "        boundary_targets = torch.full((num_points, 1), head, dtype=torch.float)\n",
    "        return boundary_points, boundary_targets\n",
    "    else:\n",
    "        return boundary_points\n",
    "\n",
    "def plot_losses(epochs, losses_dict):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for loss_name, loss_values in losses_dict.items():\n",
    "        plt.plot(epochs, loss_values, label=loss_name)\n",
    "    plt.gca().yaxis.set_major_locator(LogLocator(base=10.0))\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute individual loss terms\n",
    "    loss_laplace = laplace_residual(model, train_points)\n",
    "    loss_d_l = loss_dirichlet_boundary(model, boundary_points_dirichlet_left, target_points_dirichlet_left)\n",
    "    loss_d_r = loss_dirichlet_boundary(model, boundary_points_dirichlet_right, target_points_dirichlet_right)\n",
    "    loss_d_tl = loss_dirichlet_boundary(model, boundary_points_dirichlet_topleft, target_points_dirichlet_topleft)\n",
    "    loss_d_tr = loss_dirichlet_boundary(model, boundary_points_dirichlet_topright, target_points_dirichlet_topright)\n",
    "    loss_n_t = loss_neumann_boundary(model, boundary_points_neumann_top, axis='horizontal')\n",
    "    loss_n_b = loss_neumann_boundary(model, boundary_points_neumann_bottom, axis='horizontal')\n",
    "    \n",
    "    # Combine losses using the updated weights\n",
    "    loss = (\n",
    "        loss_weights[0] * loss_laplace +\n",
    "        loss_weights[1] * loss_d_l +\n",
    "        loss_weights[2] * loss_d_r +\n",
    "        loss_weights[3] * loss_d_tl +\n",
    "        loss_weights[4] * loss_d_tr +\n",
    "        loss_weights[5] * loss_n_t +\n",
    "        loss_weights[6] * loss_n_b\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    # Record losses and weights\n",
    "    laplace_losses.append(loss_laplace.item())\n",
    "    dirichlet_losses_left.append(loss_d_l.item())\n",
    "    dirichlet_losses_right.append(loss_d_r.item())\n",
    "    dirichlet_losses_topleft.append(loss_d_tl.item())\n",
    "    dirichlet_losses_topright.append(loss_d_tr.item())\n",
    "    neumann_losses_top.append(loss_n_t.item())\n",
    "    neumann_losses_bottom.append(loss_n_b.item())\n",
    "    epochs_recorded.append(epoch)\n",
    "    loss_recorded.append(loss.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "#_____________________________________________________________________________________________________________\n",
    "\n",
    "#Domain size:\n",
    "left, right, bottom, top = 0, 1, 0, 0.2\n",
    "\n",
    "left_dirichlet,target_left_dirichlet = create_boundary_points('vertical', left, 0, top, 1000, 1)  # Left boundary\n",
    "boundary_points_dirichlet_left=torch.cat([left_dirichlet],dim=0).requires_grad_(True)  # Concatenate Dirichlet boundary points\n",
    "target_points_dirichlet_left=torch.cat([target_left_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary targets\n",
    "\n",
    "right_dirichlet, target_right_dirichlet = create_boundary_points('vertical', right, 0, top, 1000, 0) # Right boundary\n",
    "boundary_points_dirichlet_right=torch.cat([right_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary points\n",
    "target_points_dirichlet_right=torch.cat([target_right_dirichlet],dim=0).requires_grad_(True) # Concatenate Dirichlet boundary targets\n",
    "\n",
    "topleft_dirichlet, target_topleft_dirichlet= create_boundary_points('horizontal', top, 0, 0.3*right, 3000, 1)\n",
    "boundary_points_dirichlet_topleft=torch.cat([topleft_dirichlet],dim=0).requires_grad_(True)\n",
    "target_points_dirichlet_topleft=torch.cat([target_topleft_dirichlet],dim=0).requires_grad_(True)\n",
    "\n",
    "topright_dirichlet,target_topright_dirichlet = create_boundary_points('horizontal', top, 0.7*right, 1*right, 3000, 0)\n",
    "boundary_points_dirichlet_topright=torch.cat([topright_dirichlet],dim=0).requires_grad_(True)\n",
    "target_points_dirichlet_topright=torch.cat([target_topright_dirichlet],dim=0).requires_grad_(True)\n",
    "\n",
    "top_neumann=create_boundary_points('horizontal', top, 0.3*right, 0.7*right, 3000)\n",
    "boundary_points_neumann_top=torch.cat([top_neumann],dim=0).requires_grad_(True)\n",
    "\n",
    "bottom_neumann=create_boundary_points('horizontal', bottom, 0, right, 1000)\n",
    "boundary_points_neumann_bottom=torch.cat([bottom_neumann],dim=0).requires_grad_(True)\n",
    "\n",
    "\n",
    "# Creating domain points\n",
    "num_domain_points=10000\n",
    "x_points = torch.rand(num_domain_points,1) \n",
    "x = (x_points*right).requires_grad_(True)\n",
    "y_points = torch.rand(num_domain_points,1)\n",
    "y=(y_points*top).requires_grad_(True)\n",
    "train_points=torch.cat([x,y],dim=1) \n",
    "\n",
    "# Initialize lists to store losses and plot\n",
    "laplace_losses = []\n",
    "dirichlet_losses_left = []\n",
    "dirichlet_losses_right = []\n",
    "dirichlet_losses_topleft = []\n",
    "dirichlet_losses_topright = []\n",
    "neumann_losses_top = []\n",
    "neumann_losses_bottom = []\n",
    "epochs_recorded = []\n",
    "loss_recorded = []\n",
    "\n",
    "\n",
    "#Defining hyperparameters:\n",
    "model=pinn(hidden_size=20, hidden_layers=8) \n",
    "\n",
    "epochs=3001\n",
    "\n",
    "# Initialize the loss weights\n",
    "lambda_laplace, y_d_l, y_d_r, y_d_tl, y_d_tr, y_n_t, y_n_b = 2,100,100,1000,1000,100,100\n",
    "loss_weights = (torch.tensor([lambda_laplace, y_d_l, y_d_r, y_d_tl, y_d_tr, y_n_t, y_n_b], \n",
    "                            dtype=torch.float, requires_grad=False))\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lbfgs_optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=20)\n",
    "\n",
    "# Initialize the weights and biases of the model\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)  # Use Xavier initialization\n",
    "        nn.init.zeros_(m.bias)  # Initialize biases to zero\n",
    "# Apply weight initialization\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "\n",
    "#Training loop:\n",
    "for epoch in range(epochs):\n",
    "    optimizer = adam_optimizer if epoch < 6001 else lbfgs_optimizer\n",
    "    optimizer.step(closure) # Update the weights and biases of the model\n",
    "\n",
    "    # Logging\n",
    "\n",
    "    if epoch % 2900 == 0 and epoch <= 6000:\n",
    "        losses_dict = {\n",
    "            \"Laplace Loss\": laplace_losses,\n",
    "            \"Dirichlet Loss Left\": dirichlet_losses_left,\n",
    "            \"Dirichlet Loss Right\": dirichlet_losses_right,\n",
    "            \"Dirichlet Loss Topleft\": dirichlet_losses_topleft,\n",
    "            \"Dirichlet Loss Topright\": dirichlet_losses_topright,\n",
    "            \"Neumann Loss Top\": neumann_losses_top,\n",
    "            \"Neumann Loss Bottom\": neumann_losses_bottom\n",
    "        }\n",
    "\n",
    "        plot_losses(epochs_recorded, losses_dict)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_recorded[-1]}\")\n",
    "    \n",
    "    if epoch % 100 == 0 and epoch > 2900:\n",
    "        losses_dict = {\n",
    "            \"Laplace Loss\": laplace_losses,\n",
    "            \"Dirichlet Loss Left\": dirichlet_losses_left,\n",
    "            \"Dirichlet Loss Right\": dirichlet_losses_right,\n",
    "            \"Dirichlet Loss Topleft\": dirichlet_losses_topleft,\n",
    "            \"Dirichlet Loss Topright\": dirichlet_losses_topright,\n",
    "            \"Neumann Loss Top\": neumann_losses_top,\n",
    "            \"Neumann Loss Bottom\": neumann_losses_bottom\n",
    "        }\n",
    "\n",
    "        plot_losses(epochs_recorded, losses_dict)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_recorded[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_monte_carlo_simulation(num_samples, t, m, mu, k, y0, v0, damped_harmonic_oscillator, y_threshold=-1):\n",
    "    \"\"\"\n",
    "    Runs a Monte Carlo simulation for the damped harmonic oscillator.\n",
    "    \"\"\"\n",
    "    m_samples = np.random.normal(m, 0.1, num_samples)\n",
    "    mu_samples = np.random.normal(mu, 0.05, num_samples)\n",
    "    k_samples = np.random.normal(k, 0.2, num_samples)\n",
    "    y0_samples = np.random.normal(y0, 0.0, num_samples)\n",
    "    \n",
    "    y_mc = [damped_harmonic_oscillator(m_samples[i], mu_samples[i], k_samples[i], y0_samples[i], v0, t) for i in range(num_samples)]\n",
    "    \n",
    "    y_mc = np.array(y_mc)\n",
    "    y_mean = np.mean(y_mc, axis=0)\n",
    "    y_std = np.std(y_mc, axis=0)\n",
    "    prob_exceed = np.mean(np.any(y_mc < y_threshold, axis=1))\n",
    "    \n",
    "    return y_mc, y_mean, y_std, prob_exceed\n",
    "\n",
    "def plot_monte_carlo_results(t, y_exact, y_mean, y_std):\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation of the Monte Carlo simulation.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t, y_exact, label='Exact Solution', color='red')\n",
    "    plt.plot(t, y_mean, label='Mean', color='blue')\n",
    "    plt.fill_between(t, y_mean - 3 * y_std, y_mean + 3 * y_std, color='blue', alpha=0.1, label='3 Standard Deviations')\n",
    "    plt.fill_between(t, y_mean - 2 * y_std, y_mean + 2 * y_std, color='blue', alpha=0.2, label='2 Standard Deviations')\n",
    "    plt.fill_between(t, y_mean - y_std, y_mean + y_std, color='blue', alpha=0.3, label='1 Standard Deviation')\n",
    "    plt.title('Monte Carlo Simulation of Damped Harmonic Oscillator')\n",
    "    plt.xlabel('Time (t)')\n",
    "    plt.ylabel('Displacement (y)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class ParamPINN(nn.Module):\n",
    "    def __init__(self, hidden_size=20, hidden_layers=3, dropout_rate=0.05):\n",
    "        super(ParamPINN, self).__init__()\n",
    "        input_dim = 6\n",
    "        layers = [nn.Linear(input_dim, hidden_size), nn.Tanh()]\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, t, m, mu, k, y0, v0):\n",
    "        N = t.shape[0]\n",
    "        x = torch.cat([t, m.expand(N, 1), mu.expand(N, 1), k.expand(N, 1), y0.expand(N, 1), v0.expand(N, 1)], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "def pde_loss(model, t, m, mu, k, y0_val, v0_val):\n",
    "    y = model(t, m, mu, k, y0_val, v0_val)\n",
    "    y_t = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    y_tt = torch.autograd.grad(y_t, t, grad_outputs=torch.ones_like(y_t), create_graph=True)[0]\n",
    "    residual = m * y_tt + mu * y_t + k * y\n",
    "    return torch.mean(residual**2)\n",
    "\n",
    "def boundary_loss(model, t0, m, mu, k, y0, v0):\n",
    "    y_pred = model(t0, m, mu, k, y0, v0)\n",
    "    y_t = torch.autograd.grad(y_pred, t0, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    return (y_pred - y0)**2 + (y_t - v0)**2\n",
    "\n",
    "def plot_loss(epoch, losses_dict):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    for loss_name, loss_values in losses_dict.items():\n",
    "        plt.plot(epoch, loss_values, label=loss_name)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def initialize_parameters():\n",
    "    t_coll = torch.rand(500,1)*5\n",
    "    t_coll.requires_grad_(True)\n",
    "    \n",
    "    m, mu, k = 1.0, 0.6, 5.0\n",
    "    y0, v0 = -0.4, 3.0\n",
    "    \n",
    "    m_val  = torch.tensor([[m]], requires_grad=True)\n",
    "    mu_val = torch.tensor([[mu]], requires_grad=True)\n",
    "    k_val  = torch.tensor([[k]], requires_grad=True)\n",
    "    y0_val = torch.tensor([[y0]], requires_grad=True)\n",
    "    v0_val = torch.tensor([[v0]], requires_grad=True)\n",
    "    \n",
    "    t0 = torch.tensor([0.0]).view(-1,1).requires_grad_(True)\n",
    "    \n",
    "    return t_coll, m_val, mu_val, k_val, y0_val, v0_val, t0\n",
    "\n",
    "def train_model(model, optimizer, t_coll, m_val, mu_val, k_val, y0_val, v0_val, t0, epochs=4001):\n",
    "    epoch_recorded = []\n",
    "    pde_loss_recorded = []\n",
    "    boundary_loss_recorded = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss_pde = pde_loss(model, t_coll, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "        loss_bc  = boundary_loss(model, t0, m_val, mu_val, k_val, y0_val, v0_val)\n",
    "        loss = loss_pde + 10 * loss_bc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_recorded.append(epoch)\n",
    "        pde_loss_recorded.append(loss_pde.item())\n",
    "        boundary_loss_recorded.append(loss_bc.item())\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print(f\"Epoch {epoch}, PDE loss: {loss_pde.item()}, BC loss: {loss_bc.item()}\")\n",
    "            losses_dict = {'Residual Loss': pde_loss_recorded, 'Boundary Loss': boundary_loss_recorded}\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "            plot_loss(epoch_recorded, losses_dict)\n",
    "\n",
    "# Running the training pipeline\n",
    "set_seed()\n",
    "t_coll, m_val, mu_val, k_val, y0_val, v0_val, t0 = initialize_parameters()\n",
    "model = ParamPINN(hidden_size=20, hidden_layers=3, dropout_rate=0.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_model(model, optimizer, t_coll, m_val, mu_val, k_val, y0_val, v0_val, t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Define the PINN model\n",
    "class ParamPINN(nn.Module):\n",
    "    def __init__(self, hidden_size=20, hidden_layers=3, dropout_rate=0.05):\n",
    "        super(ParamPINN, self).__init__()\n",
    "        input_dim = 6\n",
    "        layers = [nn.Linear(input_dim, hidden_size), nn.Tanh()]\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, t, m, mu, k, y0, v0):\n",
    "        N = t.shape[0]\n",
    "        mu_ = mu.expand(N, -1)\n",
    "        k_  = k.expand(N, -1)\n",
    "        y0_ = y0.expand(N, -1)\n",
    "        v0_ = v0.expand(N, -1)\n",
    "        x = torch.cat([t, m, mu_, k_, y0_, v0_], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Compute PDE loss\n",
    "def pde_loss(model, t, m, mu, k, y0_val, v0_val):\n",
    "    y = model(t, m, mu, k, y0_val, v0_val)\n",
    "    y_t = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    y_tt = torch.autograd.grad(y_t, t, grad_outputs=torch.ones_like(y_t), create_graph=True)[0]\n",
    "    residual = m * y_tt + mu * y_t + k * y\n",
    "    return torch.mean(residual**2)\n",
    "\n",
    "# Compute boundary loss\n",
    "def boundary_loss(model, t0, m, mu, k, y0, v0):\n",
    "    y_pred = model(t0, m, mu, k, y0, v0)\n",
    "    y_t = torch.autograd.grad(y_pred, t0, grad_outputs=torch.ones_like(y_pred), create_graph=True)[0]\n",
    "    return torch.mean((y_pred - y0)**2 + (y_t - v0)**2)\n",
    "\n",
    "# Plot loss function\n",
    "def plot_loss(epoch, losses_dict):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    for loss_name, loss_values in losses_dict.items():\n",
    "        plt.plot(epoch, loss_values, label=loss_name)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize training parameters\n",
    "def initialize_parameters():\n",
    "    N = 500\n",
    "    t_coll = torch.rand(N, 1) * 5\n",
    "    t_coll.requires_grad_(True)\n",
    "    m_mean, m_std = 2.0, 0.2\n",
    "    m_coll = m_mean + m_std * torch.randn((N, 1)) # Sample from standard normal and scale\n",
    "    m_coll.requires_grad_(True) # Set requires_grad to True for backpropagation\n",
    "    mu, k, y0, v0 = 0.6, 5.0, -0.4, 3.0\n",
    "    # Define parameters as scalars (shape (1, 1))\n",
    "    mu_val = torch.tensor([[mu]], requires_grad=True)\n",
    "    k_val = torch.tensor([[k]], requires_grad=True)\n",
    "    y0_val = torch.tensor([[y0]], requires_grad=True)\n",
    "    v0_val = torch.tensor([[v0]], requires_grad=True)\n",
    "\n",
    "    # Also define a single time for boundary condition\n",
    "    t0 = torch.zeros_like(t_coll).clone().detach().requires_grad_(True)\n",
    "\n",
    "    return t_coll, m_coll, mu_val, k_val, y0_val, v0_val, t0\n",
    "\n",
    "# Trainer class to manage training process\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, epochs=4001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.losses = {\"Residual Loss\": [], \"Boundary Loss\": []}\n",
    "        self.lambda_bc = 10.0\n",
    "    \n",
    "    def train(self, t_coll, m_coll, mu_val, k_val, y0_val, v0_val, t0):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_pde = pde_loss(self.model, t_coll, m_coll, mu_val, k_val, y0_val, v0_val)\n",
    "            loss_bc = boundary_loss(self.model, t0, m_coll, mu_val, k_val, y0_val, v0_val)\n",
    "            loss = loss_pde + self.lambda_bc * loss_bc\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losses[\"Residual Loss\"].append(loss_pde.item())\n",
    "            self.losses[\"Boundary Loss\"].append(loss_bc.item())\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, PDE loss: {loss_pde.item()}, BC loss: {loss_bc.item()}\")\n",
    "                plot_loss(range(epoch + 1), self.losses)\n",
    "\n",
    "# Run training pipeline\n",
    "set_seed()\n",
    "t_coll, m_coll, mu_val, k_val, y0_val, v0_val, t0 = initialize_parameters()\n",
    "model = ParamPINN(hidden_size=20, hidden_layers=3, dropout_rate=0.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.train(t_coll, m_coll, mu_val, k_val, y0_val, v0_val, t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The system is not underdamped. Ensure μ^2 < 4 * m * k.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Importance sampling approach\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m p_fail_is \u001b[38;5;241m=\u001b[39m \u001b[43mIS_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance sampling estimate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_fail_is\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 68\u001b[0m, in \u001b[0;36mIS_sim\u001b[1;34m(N, t, threshold)\u001b[0m\n\u001b[0;32m     65\u001b[0m     y0_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 3) Evaluate failure: does y(t) < threshold at ANY time?\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mdamped_harmonic_oscillator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mk_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# if any point is below threshold, it's a 'failure'\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     fail \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(y \u001b[38;5;241m<\u001b[39m threshold)\n",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m, in \u001b[0;36mdamped_harmonic_oscillator\u001b[1;34m(t, m, mu, k, y0, v0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdamped_harmonic_oscillator\u001b[39m(t, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m, y0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.4\u001b[39m, v0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Check underdamped condition\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mu\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m m \u001b[38;5;241m*\u001b[39m k:\n\u001b[1;32m----> 9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe system is not underdamped. Ensure μ^2 < 4 * m * k.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     omega \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(k\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m-\u001b[39m (mu\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mm))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     13\u001b[0m     A \u001b[38;5;241m=\u001b[39m y0\n",
      "\u001b[1;31mValueError\u001b[0m: The system is not underdamped. Ensure μ^2 < 4 * m * k."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# -- 1) EXACT SOLUTION FOR THE OSCILLATOR --\n",
    "def damped_harmonic_oscillator(t, m=1.0, mu=0.6, k=5.0, y0=-0.4, v0=3.0):\n",
    "    # Check underdamped condition\n",
    "    if mu**2 >= 4 * m * k:\n",
    "        raise ValueError(\"The system is not underdamped. Ensure μ^2 < 4 * m * k.\")\n",
    "    \n",
    "    omega = np.sqrt(k/m - (mu/(2*m))**2)\n",
    "    \n",
    "    A = y0\n",
    "    B = (v0 + (mu/(2*m)) * y0) / omega\n",
    "    y_exact = np.exp(-mu * t / (2*m)) * (A * np.cos(omega * t) + B * np.sin(omega * t))\n",
    "    return y_exact\n",
    "\n",
    "\n",
    "# -- 2) PDF FUNCTIONS FOR p(x) AND g(x) --\n",
    "def p_pdf(m, mu, k):\n",
    "    \"\"\" PDF of the ORIGINAL distribution (product of independent normals).\n",
    "        m ~ N(1, 0.1^2), mu ~ N(0.6, 0.05^2), k ~ N(5, 0.2^2), y0 ~ N(-0.4, 0^2)\n",
    "    \"\"\"\n",
    "    pm = norm.pdf(m, loc=1.0, scale=0.1)\n",
    "    pmu = norm.pdf(mu, loc=0.6, scale=0.05)\n",
    "    pk = norm.pdf(k, loc=5.0, scale=0.2)\n",
    "    return pm * pmu * pk\n",
    "\n",
    "def g_pdf(m, mu, k):\n",
    "    \"\"\" PDF of the PROPOSAL distribution g(x).\n",
    "        Adjust means or std to focus on \"dangerous\" region, e.g. larger mass or smaller damping\n",
    "    \"\"\"\n",
    "    # Example: shift mass to a higher mean (like 1.2) and damping to a smaller mean (like 0.5)\n",
    "    gm = norm.pdf(m, loc=1.8, scale=0.2)\n",
    "    gmu = norm.pdf(mu, loc=0.5, scale=0.05)\n",
    "    gk = norm.pdf(k, loc=5.0, scale=0.3)\n",
    "    return gm * gmu * gk\n",
    "\n",
    "def sample_from_g(N):\n",
    "    \"\"\" Draw N samples from the proposal distribution g(x). \"\"\"\n",
    "    m_samples = np.random.normal(loc=1.8, scale=0.2, size=N)\n",
    "    mu_samples = np.random.normal(loc=0.5, scale=0.05, size=N)\n",
    "    k_samples = np.random.normal(loc=5.0, scale=0.3, size=N)\n",
    "    return m_samples, mu_samples, k_samples\n",
    "\n",
    "\n",
    "# -- 3) IMPORTANCE SAMPLING --\n",
    "def IS_sim(N, t, threshold=-1.0):\n",
    "    \"\"\"\n",
    "    Estimate probability that oscillator crosses below 'threshold'\n",
    "    at ANY time in [t.min(), t.max()] using importance sampling.\n",
    "    \"\"\"\n",
    "    # 1) Sample from g(x)\n",
    "    m_samples, mu_samples, k_samples = sample_from_g(N)\n",
    "\n",
    "    # 2) Evaluate weights w_i = p(x_i) / g(x_i)\n",
    "    w = []\n",
    "    indicators = []\n",
    "    for i in range(N):\n",
    "        # Evaluate ratio\n",
    "        px = p_pdf(m_samples[i], mu_samples[i], k_samples[i] )\n",
    "        gx = g_pdf(m_samples[i], mu_samples[i], k_samples[i] )\n",
    "        w.append(px / gx if gx > 1e-30 else 0.0)\n",
    "\n",
    "        y0_samples = np.full(N, -0.4)\n",
    "\n",
    "    # 3) Evaluate failure: does y(t) < threshold at ANY time?\n",
    "        y = damped_harmonic_oscillator(t, m_samples[i], mu_samples[i],\n",
    "                                       k_samples[i], y0_samples[i], v0=3.0)\n",
    "        # if any point is below threshold, it's a 'failure'\n",
    "        fail = np.any(y < threshold)\n",
    "        indicators.append(fail)\n",
    "\n",
    "    w = np.array(w)\n",
    "    indicators = np.array(indicators, dtype=float)\n",
    "\n",
    "    # 4) Weighted average for probability\n",
    "    # p_fail = (1/N) * sum( I{fail} * w_i )\n",
    "    p_fail = np.mean(indicators * w)\n",
    "\n",
    "    return p_fail\n",
    "\n",
    "# 4) Run and compare with standard MC\n",
    "if __name__ == \"__main__\":\n",
    "    # np.random.seed(0)\n",
    "    t = np.linspace(0, 5, 100)\n",
    "    N = 10000\n",
    "\n",
    "    # Importance sampling approach\n",
    "    p_fail_is = IS_sim(N, t, threshold=-1.0)\n",
    "\n",
    "    print(f\"Importance sampling estimate: {p_fail_is}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 IS estimate: 0.5350870537496035\n",
      "Number of failure samples in Round 1: 9433\n",
      "Adaptive proposal parameters:\n",
      "m: mean = 1.224641597364364 , std = 0.18021771010463986\n",
      "mu: mean = 0.49812559096072023 , std = 0.049947544328596376\n",
      "k: mean = 4.998727350928771 , std = 0.30097359080327485\n",
      "Round 2 (adaptive) IS estimate: 0.519819600534585\n",
      "Final adaptive IS probability estimate: 0.519819600534585\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Exact solution for the damped oscillator\n",
    "def damped_harmonic_oscillator(t, m=1.0, mu=0.6, k=5.0, y0=-0.4, v0=3.0):\n",
    "    if mu**2 >= 4 * m * k:\n",
    "        raise ValueError(\"The system is not underdamped. Ensure μ^2 < 4*m*k.\")\n",
    "    omega = np.sqrt(k/m - (mu/(2*m))**2)\n",
    "    A = y0\n",
    "    B = (v0 + (mu/(2*m)) * y0) / omega\n",
    "    y_exact = np.exp(-mu * t / (2*m)) * (A * np.cos(omega * t) + B * np.sin(omega * t))\n",
    "    return y_exact\n",
    "\n",
    "# Original target distribution p(x)\n",
    "def p_pdf(m, mu, k, y0):\n",
    "    pm = norm.pdf(m, loc=1.0, scale=0.1)\n",
    "    pmu = norm.pdf(mu, loc=0.6, scale=0.05)\n",
    "    pk = norm.pdf(k, loc=5.0, scale=0.2)\n",
    "    py0 = 1.0  # y0 is fixed at -0.4\n",
    "    return pm * pmu * pk * py0\n",
    "\n",
    "# Initial proposal distribution g(x)\n",
    "def sample_from_g_initial(N):\n",
    "    # Choose a broad distribution to cover a wide range\n",
    "    m_samples = np.random.normal(loc=1.2, scale=0.2, size=N)\n",
    "    mu_samples = np.random.normal(loc=0.5, scale=0.05, size=N)\n",
    "    k_samples = np.random.normal(loc=5.0, scale=0.3, size=N)\n",
    "    y0_samples = np.full(N, -0.4)\n",
    "    return m_samples, mu_samples, k_samples, y0_samples\n",
    "\n",
    "def g_pdf_initial(m, mu, k, y0):\n",
    "    gm = norm.pdf(m, loc=1.2, scale=0.2)\n",
    "    gmu = norm.pdf(mu, loc=0.5, scale=0.05)\n",
    "    gk = norm.pdf(k, loc=5.0, scale=0.3)\n",
    "    gy0 = 1.0\n",
    "    return gm * gmu * gk * gy0\n",
    "\n",
    "# Adaptive Importance Sampling\n",
    "def adaptive_IS_sim(N, t, threshold=-1.0):\n",
    "    # ------ Round 1: Initial Proposal ------\n",
    "    m_samples, mu_samples, k_samples, y0_samples = sample_from_g_initial(N)\n",
    "    weights_round1 = []\n",
    "    indicators_round1 = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        px = p_pdf(m_samples[i], mu_samples[i], k_samples[i], y0_samples[i])\n",
    "        gx = g_pdf_initial(m_samples[i], mu_samples[i], k_samples[i], y0_samples[i])\n",
    "        weight = px / gx if gx > 1e-30 else 0.0\n",
    "        weights_round1.append(weight)\n",
    "        y = damped_harmonic_oscillator(t, m_samples[i], mu_samples[i], k_samples[i], y0_samples[i], v0=3.0)\n",
    "        fail = np.any(y < threshold)\n",
    "        indicators_round1.append(fail)\n",
    "    \n",
    "    weights_round1 = np.array(weights_round1)\n",
    "    indicators_round1 = np.array(indicators_round1, dtype=float)\n",
    "    p_fail_round1 = np.mean(indicators_round1 * weights_round1)\n",
    "    print(\"Round 1 IS estimate:\", p_fail_round1)\n",
    "    \n",
    "    # Identify failure samples from round 1\n",
    "    failure_indices = np.where(indicators_round1 == 1)[0]\n",
    "    print(\"Number of failure samples in Round 1:\", len(failure_indices))\n",
    "    if len(failure_indices) == 0:\n",
    "        print(\"No failures in Round 1; cannot adapt proposal.\")\n",
    "        return p_fail_round1\n",
    "    \n",
    "    # ------ Fit Adaptive Proposal based on failure samples ------\n",
    "    m_fail = m_samples[failure_indices]\n",
    "    mu_fail = mu_samples[failure_indices]\n",
    "    k_fail = k_samples[failure_indices]\n",
    "    \n",
    "    # Compute the mean and std for failure samples\n",
    "    m_mean, m_std = np.mean(m_fail), np.std(m_fail)\n",
    "    mu_mean, mu_std = np.mean(mu_fail), np.std(mu_fail)\n",
    "    k_mean, k_std = np.mean(k_fail), np.std(k_fail)\n",
    "    \n",
    "    print(\"Adaptive proposal parameters:\")\n",
    "    print(\"m: mean =\", m_mean, \", std =\", m_std)\n",
    "    print(\"mu: mean =\", mu_mean, \", std =\", mu_std)\n",
    "    print(\"k: mean =\", k_mean, \", std =\", k_std)\n",
    "    \n",
    "    # Define adaptive proposal sampling functions using the fitted parameters\n",
    "    def sample_from_g_adaptive(N):\n",
    "        m_samples = np.random.normal(loc=m_mean, scale=m_std, size=N)\n",
    "        mu_samples = np.random.normal(loc=mu_mean, scale=mu_std, size=N)\n",
    "        k_samples = np.random.normal(loc=k_mean, scale=k_std, size=N)\n",
    "        y0_samples = np.full(N, -0.4)\n",
    "        return m_samples, mu_samples, k_samples, y0_samples\n",
    "    \n",
    "    def g_pdf_adaptive(m, mu, k, y0):\n",
    "        gm = norm.pdf(m, loc=m_mean, scale=m_std)\n",
    "        gmu = norm.pdf(mu, loc=mu_mean, scale=mu_std)\n",
    "        gk = norm.pdf(k, loc=k_mean, scale=k_std)\n",
    "        gy0 = 1.0\n",
    "        return gm * gmu * gk * gy0\n",
    "    \n",
    "    # ------ Round 2: Adaptive Proposal ------\n",
    "    m_samples2, mu_samples2, k_samples2, y0_samples2 = sample_from_g_adaptive(N)\n",
    "    weights_round2 = []\n",
    "    indicators_round2 = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        px = p_pdf(m_samples2[i], mu_samples2[i], k_samples2[i], y0_samples2[i])\n",
    "        gx = g_pdf_adaptive(m_samples2[i], mu_samples2[i], k_samples2[i], y0_samples2[i])\n",
    "        weight = px / gx if gx > 1e-30 else 0.0\n",
    "        weights_round2.append(weight)\n",
    "        y = damped_harmonic_oscillator(t, m_samples2[i], mu_samples2[i], k_samples2[i], y0_samples2[i], v0=3.0)\n",
    "        fail = np.any(y < threshold)\n",
    "        indicators_round2.append(fail)\n",
    "    \n",
    "    weights_round2 = np.array(weights_round2)\n",
    "    indicators_round2 = np.array(indicators_round2, dtype=float)\n",
    "    p_fail_round2 = np.mean(indicators_round2 * weights_round2)\n",
    "    print(\"Round 2 (adaptive) IS estimate:\", p_fail_round2)\n",
    "    \n",
    "    return p_fail_round2\n",
    "\n",
    "# Run the adaptive IS simulation\n",
    "if __name__ == \"__main__\":\n",
    "    #np.random.seed(0)\n",
    "    t = np.linspace(0, 5, 100)\n",
    "    N = 10000\n",
    "    p_fail_adaptive = adaptive_IS_sim(N, t, threshold=-0.696)\n",
    "    print(\"Final adaptive IS probability estimate:\", p_fail_adaptive)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
