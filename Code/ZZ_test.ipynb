{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "EPS = 1e-5  # define a small constant for numerical stability control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_func(x):\n",
    "    return np.sin(x * math.pi / 2 + 0.8) * np.exp(-0.1 * np.abs(x)) + 0.1 * x\n",
    "\n",
    "def gen_data(N_data, ground_truth_func, noise_std=None): \n",
    "    # generate the training dataset, note here we will make data into 2 clusters\n",
    "    x1 = np.random.randn(int(N_data/2), 1) * 0.5 + 2.0\n",
    "    x2 = np.random.randn(int(N_data/2), 1) * 0.5 - 2.0\n",
    "    x = np.concatenate([x1, x2], axis=0)\n",
    "    y = ground_truth_func(x)\n",
    "    if noise_std is not None and noise_std > EPS:\n",
    "        # assume homogeneous noise setting, i.e., \"homoscedasticity\"\n",
    "        y += np.random.randn(y.shape[0], y.shape[1]) * noise_std\n",
    "    return x, y\n",
    "\n",
    "def normalise_data(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "def unnormalise_data(x, mean, std):\n",
    "    return x * std + mean\n",
    "\n",
    "class regression_data(Dataset):\n",
    "     def __init__(self, x, y, normalise=True):\n",
    "         super(regression_data, self).__init__()\n",
    "         self.update_data(x, y, normalise)\n",
    "\n",
    "     def __len__(self):\n",
    "         return self.x.shape[0]\n",
    "\n",
    "     def __getitem__(self, index):\n",
    "         x = torch.tensor(self.x[index]).float()\n",
    "         y = torch.tensor(self.y[index]).float()\n",
    "         return x, y\n",
    "\n",
    "     def update_data(self, x, y, normalise=True, update_stats=True):\n",
    "         assert x.shape[0] == y.shape[0]\n",
    "         self.x = x\n",
    "         self.y = y\n",
    "         # normalise data\n",
    "         self.normalise = normalise\n",
    "         if update_stats:\n",
    "             self.x_mean = self.x.mean(0) if normalise else 0.0\n",
    "             self.x_std = self.x.std(0) if normalise else 1.0\n",
    "             self.y_mean = self.y.mean(0) if normalise else 0.0\n",
    "             self.y_std = self.y.std(0) if normalise else 1.0\n",
    "         if self.normalise:\n",
    "             self.x = normalise_data(self.x, self.x_mean, self.x_std)\n",
    "             self.y = normalise_data(self.y, self.y_mean, self.y_std)\n",
    "\n",
    "N_data = 100\n",
    "noise_std = 0.1\n",
    "x_train, y_train = gen_data(N_data, ground_truth_func, noise_std)\n",
    "dataset = regression_data(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# plot the training data and ground truth\n",
    "x_test = np.arange(np.min(x_train) - 1.0, np.max(x_train)+1.0, 0.01)[:, np.newaxis]\n",
    "y_test = ground_truth_func(x_test)\n",
    "plt.plot(x_train, y_train, 'ro', label='data')\n",
    "plt.plot(x_test, y_test, 'k-', label='ground-truth')\n",
    "plt.legend()\n",
    "plt.title('ground-truth function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    # y should have shape as (batch_size, y_dim)\n",
    "    # y_pred should have shape as (batch_size, y_dim) or (K, batch_size, y_dim)\n",
    "    # where K is the number of MC samples\n",
    "    # this function should return per-data loss of shape (batch_size,) or (K, batch_size)\n",
    "    ### begin of your code ###\n",
    "    # hint: consult with your textbook or wikipedia for the Gaussian distribution form\n",
    "    l2_dist=(y-y_pred).pow(2).sum(-1)\n",
    "    ll = -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "    ### end of your code ###\n",
    "    return ll\n",
    "\n",
    "# we assume a Gaussian likelihood with homogeneuous noise\n",
    "log_noise_var = nn.Parameter(torch.ones(size=(), device=device)*-3.0)\n",
    "\n",
    "data_loss_func = lambda y, y_pred: -gauss_loglik(y, y_pred, log_noise_var)\n",
    "nll = data_loss_func(y, y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib.ticker import LogLocator, FormatStrFormatter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
