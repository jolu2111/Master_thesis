{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib.ticker import LogLocator, FormatStrFormatter\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "EPS = 1e-5  # define a small constant for numerical stability control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importance sampling during training\n",
    "\n",
    "Below is an example implementation of a PINN in PyTorch that incorporates self-adaptive importance sampling similar to the paper. In this simplified example, we solve a one-dimensional PDE (here, we use a toy example u_xx +sin(Ï€x)=0) by training on collocation points that are periodically updated by sampling more densely in regions where the PDE residual is high.\n",
    "\n",
    "You can adjust the PDE, network architecture, and hyperparameters as needed.\n",
    "\n",
    "### Sampling Strategy:\n",
    "\n",
    "Initially, collocation points are sampled uniformly.\n",
    "In the function importance_sampling, the model is evaluated on a dense grid over the domain. The absolute value of the residual (plus a small epsilon) forms a weight distribution, and new collocation points are drawn according to this distribution. This mimics the failure-informed sampling by focusing training on regions where the PDE is not well satisfied.\n",
    "### Training Loop:\n",
    "The model trains for a set number of epochs. Every few epochs (here, every 1000 epochs), the collocation points are updated using the importance sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network for the PINN\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Create a fully connected network based on the provided layers list.\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            a = self.activation(layer(a))\n",
    "        output = self.layers[-1](a)\n",
    "        return output\n",
    "\n",
    "# Define the PDE residual\n",
    "def pde_residual(model, x):\n",
    "    # Enable gradient tracking on x\n",
    "    x.requires_grad = True\n",
    "    u = model(x)\n",
    "    # Compute first derivative u_x\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    # Compute second derivative u_xx\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    # For this example, our PDE is: u_xx + sin(pi*x) = 0\n",
    "    residual = u_xx + torch.sin(np.pi * x)\n",
    "    return residual\n",
    "\n",
    "# Generate initial collocation points uniformly in the domain [lb, ub]\n",
    "def generate_collocation_points(n_points, lb, ub):\n",
    "    x = np.random.uniform(lb, ub, (n_points, 1))\n",
    "    return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# Importance sampling: Evaluate the residual on a fine grid and sample new points\n",
    "def importance_sampling(model, n_points, lb, ub):\n",
    "    # Create a fine grid over the domain\n",
    "    x_fine = np.linspace(lb, ub, 1000).reshape(-1, 1)\n",
    "    x_fine_tensor = torch.tensor(x_fine, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the residual on the fine grid\n",
    "    residual = pde_residual(model, x_fine_tensor)\n",
    "    res_val = residual.detach().abs().squeeze().numpy()  # absolute error\n",
    "    # Avoid zero probabilities by adding a small epsilon\n",
    "    eps = 1e-6\n",
    "    weights = res_val + eps\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Sample indices based on the residual weights\n",
    "    indices = np.random.choice(len(x_fine), size=n_points, replace=True, p=weights)\n",
    "    sampled_points = x_fine[indices]\n",
    "    return torch.tensor(sampled_points, dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters and domain setup\n",
    "lb, ub = 0.0, 1.0         # Domain boundaries\n",
    "n_initial = 100           # Number of initial collocation points\n",
    "n_epochs = 1000           # Total training epochs\n",
    "sampling_interval = 200  # How often (in epochs) to update the collocation points\n",
    "learning_rate = 1e-3      # Learning rate\n",
    "\n",
    "# Define the network architecture (input, hidden layers, output)\n",
    "layers = [1, 50, 50, 1]\n",
    "model = PINN(layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Generate the initial set of collocation points\n",
    "collocation_points = generate_collocation_points(n_initial, lb, ub)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the residual of the PDE at the collocation points\n",
    "    residual = pde_residual(model, collocation_points)\n",
    "    loss_res = mse_loss(residual, torch.zeros_like(residual))\n",
    "    \n",
    "    # (Optional) Include boundary conditions loss here.\n",
    "    # For example, enforcing u(0)=0 and u(1)=0:\n",
    "    x_bc = torch.tensor([[lb], [ub]], dtype=torch.float32)\n",
    "    u_bc = model(x_bc)\n",
    "    loss_bc = mse_loss(u_bc, torch.zeros_like(u_bc))\n",
    "    \n",
    "    loss = loss_res + loss_bc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Every sampling_interval epochs, perform importance sampling to update collocation points.\n",
    "    if (epoch + 1) % sampling_interval == 0:\n",
    "        collocation_points = importance_sampling(model, n_initial, lb, ub)\n",
    "        print(f\"Resampled collocation points at epoch {epoch+1}\")\n",
    "\n",
    "# After training, evaluate the model on a fine grid and plot the results.\n",
    "x_fine = np.linspace(lb, ub, 1000).reshape(-1, 1)\n",
    "x_fine_tensor = torch.tensor(x_fine, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_fine_tensor).detach().cpu().numpy()\n",
    "\n",
    "# Exact solution: u(x) = (1/pi^2)*sin(pi*x) with u(0)=u(1)=0.\n",
    "u_exact = (1/np.pi**2) * np.sin(np.pi * x_fine)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_fine, u_pred, label='PINN Approximation', linewidth=2)\n",
    "plt.plot(x_fine, u_exact, 'r--', label='Exact Solution', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.legend()\n",
    "plt.title('PINN Approximation vs Exact Solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate g(x) and residual for each sample\n",
    "def evaluate_G_and_residual(PINN_model, samples, t_coll, params,lambda_bc=2.0):\n",
    "\n",
    "    t0_tensor =torch.zeros_like(t_coll).requires_grad_(True)\n",
    "    G_values = []\n",
    "    residual_values = []\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        row = samples[i]\n",
    "        input_params = make_input_params(t_coll, row, params['norm_info'])\n",
    "\n",
    "        # Ensure tensors require gradients\n",
    "        m_tensor = input_params[0].requires_grad_(True)\n",
    "        mu_tensor = input_params[1].requires_grad_(True)\n",
    "        k_tensor = input_params[2].requires_grad_(True)\n",
    "        y0_tensor = input_params[3].requires_grad_(True)\n",
    "        v0_tensor = input_params[4].requires_grad_(True)\n",
    "        \n",
    "        # Evaluate the limit state function G(x) for each sample\n",
    "        G_values.append(limit_state_function_G(PINN_model, t_coll, input_params, differentiable=False).item())\n",
    "\n",
    "        # Compute the residual\n",
    "        pde_residual = pde_loss(PINN_model, t_coll.requires_grad_(True), m_tensor, mu_tensor, k_tensor, y0_tensor, v0_tensor, norm_info=params['norm_info']).item()\n",
    "        # Compute the boundary loss\n",
    "        boundary_residual = boundary_loss(PINN_model, t0_tensor, m_tensor, mu_tensor, k_tensor, y0_tensor, v0_tensor, norm_info=params['norm_info']).item()\n",
    "        residual_values.append(pde_residual + lambda_bc * boundary_residual)\n",
    "\n",
    "    # Convert to NumPy arrays for sampling\n",
    "    # Not really needed, but for more versatile use it is better to have them in numpy format\n",
    "    G_vals = np.array(G_values)\n",
    "    residual_vals = np.array(residual_values)\n",
    "    return G_vals, residual_vals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
